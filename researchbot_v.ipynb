{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from pprint import pprint\n",
    "from typing_extensions import TypedDict\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from semanticscholar import SemanticScholar\n",
    "import google.generativeai as genai\n",
    "from google.oauth2 import service_account\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path=\"API_key.env\")\n",
    "service_account_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "credentials = service_account.Credentials.from_service_account_file(service_account_key)\n",
    "genai.configure(credentials=credentials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch = SemanticScholar(timeout=15)\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt\n",
    "\n",
    "\n",
    "@retry(wait=wait_fixed(5), stop=stop_after_attempt(5))\n",
    "\n",
    "def search_semantic_scholar(query: str, year: tuple[int, int]) -> list:\n",
    "    return sch.search_paper(\n",
    "        query=query, \n",
    "        year=f\"{year[0]}-{year[1]}\", \n",
    "        open_access_pdf=True,\n",
    "        fields_of_study=[\"Computer Science\"], \n",
    "        fields=[\"paperId\", \"title\", \"abstract\", \"tldr\", \"openAccessPdf\"],\n",
    "        limit=15,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "You are a Computer Science PhD student. Your goal is to write review/survey papers in specific areas of Computer Science. You should be able to:\n",
    "- Identify research problems and break them down into sub problems\n",
    "- Conduct thorough literature review on your topic, summarize key findings and identify gaps in existing methodologies\n",
    "- Formulate clear and testable hypotheses to address your research questions\n",
    "- Develop experimental methodologies to test your hypotheses, considering factors such as data collection, analysis, and evaluation\n",
    "- Collect, clean and analyze relevant data using appropriate tools and techniques\n",
    "- Draw meaningful conclusions from your research findings and discuss their implications\n",
    "- Prepare high-quality research papers that effectively communicate your findings\n",
    "\"\"\"\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", system_instruction=system_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Subproblem(TypedDict):\n",
    "#     prompt: str\n",
    "#     requires_internet: bool\n",
    "#     requires_previous_output: bool\n",
    "\n",
    "\n",
    "topic = \"Image classification\"\n",
    "prompt = f\"\"\"\n",
    "You are researching the below topic and need to write a survey paper on the same. Your current goal is only to research on the topic \n",
    "and not write anything currently.\n",
    "\n",
    "Topic: {topic}\n",
    "\n",
    "Instructions:\n",
    "- Identify the key areas of focus within this topic and outline the subproblems that need to be addressed\n",
    "- For each subproblem, create a concise prompt that states the task to be performed\n",
    "- Indicate whether internet access is necessary to complete the subproblem. Assume that around 15-20 relevant research papers will be provided to you.\n",
    "- Determine if the output of the previous subproblem is relevant to the subsequent subproblem\n",
    "- Your first subproblem should always be a query string that can be used to find relevant research papers from the Semantic Scholar database\n",
    "\n",
    "Do not generate the same instructions as your output. Ensure that you provide relevant subproblems that can be addressed by you.\n",
    "Ensure that your output is in the correct format since it will be parsed automatically.\n",
    "\"\"\"\n",
    "\n",
    "# Define a simplified schema manually\n",
    "gen_config = {\n",
    "    \"response_mime_type\": \"application/json\",\n",
    "    \"response_schema\": {\n",
    "        \"type\": \"array\",\n",
    "        \"items\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"prompt\": {\"type\": \"string\"},\n",
    "                \"requires_internet\": {\"type\": \"boolean\"},\n",
    "                \"requires_previous_output\": {\"type\": \"boolean\"}\n",
    "            },\n",
    "            \"required\": [\"prompt\", \"requires_internet\", \"requires_previous_output\"]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Generate the response using the model\n",
    "response = model.generate_content(prompt, generation_config=gen_config)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clothing Genre Recognition System Using Image Processing Techniques- A Survey\n",
      "Survey Towards Android Application for Plant Disease Detection using Deep Learning Approach\n",
      "A REVIEW ON CONTENT BASED IMAGE RETRIEVAL\n",
      "A Melting Pot of Evolution and Learning\n",
      "Multi-Label Active Learning Algorithms for Image Classification\n",
      "Survey on Clustering Techniques for Image Categorization Dataset\n",
      "Content-based image retrieval for fabric images: A survey\n",
      "Deep Active Learning in the Presence of Label Noise: A Survey\n",
      "A survey on sentiment classification algorithms, challenges and applications\n",
      "Deep CNN and Deep GAN in Computational Visual Perception-Driven Image Analysis\n",
      "Predicting Survival in Patients with Brain Tumors: Current State-of-the-Art of AI Methods Applied to MRI\n",
      "Special issue on real-time image and video processing in mobile embedded systems\n",
      "Image Retrieval: Modelling Keywords via Low-level Features\n",
      "Active Learning to Assist Annotation of Aerial Images in Environmental Surveys\n",
      "gamma-sky.net: Portal to the Gamma-Ray Sky\n",
      "                                                Paper  Cluster\n",
      "0   Clothing Genre Recognition System Using Image ...        1\n",
      "1   Survey Towards Android Application for Plant D...        2\n",
      "2   A REVIEW ON CONTENT BASED IMAGE RETRIEVAL Lite...        0\n",
      "3   A Melting Pot of Evolution and Learning We sur...        1\n",
      "4   Multi-Label Active Learning Algorithms for Ima...        1\n",
      "5   Survey on Clustering Techniques for Image Cate...        0\n",
      "6   Content-based image retrieval for fabric image...        0\n",
      "7   Deep Active Learning in the Presence of Label ...        1\n",
      "8   A survey on sentiment classification algorithm...        1\n",
      "9   Deep CNN and Deep GAN in Computational Visual ...        1\n",
      "10  Predicting Survival in Patients with Brain Tum...        1\n",
      "11  Special issue on real-time image and video pro...        0\n",
      "12  Image Retrieval: Modelling Keywords via Low-le...        0\n",
      "13  Active Learning to Assist Annotation of Aerial...        1\n",
      "14  gamma-sky.net: Portal to the Gamma-Ray Sky thi...        1\n",
      "\n",
      "Cluster 0:\n",
      "- A REVIEW ON CONTENT BASED IMAGE RETRIEVAL Literature survey is an important for understanding and gaining much more knowledge about the specific area of a subject. An image retrieval system is a computer system for browsing, searching and retrieving images from a large database of digital images. Content-based image retrieval (CBIR) is an image search technique that complements the traditional text-based retrieval of images by using visual features, such as color, texture, and shape. This system retrieve according to the query image; that is, the user provides or selects a query image and chooses a distance measure that will be used to compare the query image to the images stored in the database. This paper is attempt to explore different CBIR technique and their application. General Terms Content based image retrieval(CBIR), color, shape, texture, feature vector, classification, principle of CBIR, performance parameter\n",
      "- Survey on Clustering Techniques for Image Categorization Dataset Content Based Image Retrieval, CBIR, performed an automated classification task for a queried image. It could relieve a user from the laborious and time-consuming metadata assigning for an image while working on massive image collection. For an image, user’s definition or description is subjective where it could belong to different categories as defined by different users. Human based categorization and computer-based categorization might produce different results due to different categorization criteria that rely on dataset structure and the clustering techniques. This paper is aimed to exhibit an idea for planning the dataset structure and choosing the clustering algorithm for CBIR implementation. There are 5 sections arranged in this paper; CBIR and QBE con-cepts are introduced in Section 1, related image categorization research is listed in Section 2, the 5 type of image clustering are described in Section 3, comparative analysis in Section 4, and Section 5 conclude this study. Outcome of this paper will be benefiting CBIR developer for various applica-tions.\n",
      "- Content-based image retrieval for fabric images: A survey In recent years, a great deal of research has been conducted in the area of fabric image retrieval, especially the identification and classification of visual features. One of the challenges associated with the domain of content-based image retrieval (CBIR) is the semantic gap between low-level visual features and high-level human perceptions. Generally, CBIR includes two main components, namely feature extraction and similarity measurement. Therefore, this research aims to determine the content-based image retrieval for fabric using feature extraction techniques grouped into traditional methods and convolutional neural networks (CNN). Traditional descriptors deal with low-level features, while CNN addresses the high-level, called semantic features. Traditional descriptors have the advantage of shorter computation time and reduced system requirements. Meanwhile, CNN descriptors, which handle high-level features tailored to human perceptions, deal with large amounts of data and require a great deal of computation time. In general, the features of a CNN's fully connected layers are used for matching query and database images. In several studies, the extracted features of the CNN's convolutional layer were used for image retrieval. At the end of the CNN layer, hash codes are added to reduce  search time.\n",
      "- Special issue on real-time image and video processing in mobile embedded systems None\n",
      "- Image Retrieval: Modelling Keywords via Low-level Features With the advent of cheap digital recording and storage devices and the rapidly increasing popularity of online social networks that make extended use of visual information, like Facebook and Instagram, image retrieval regained great attention among the researchers in the areas of image indexing and retrieval. Image retrieval methods are mainly falling into content-based and text-based frameworks. Although content-based image retrieval has attracted large amount of research interest, the difficulties in querying by an example propel ultimate users towards text queries. Searching by text queries yields more effective and accurate results that meet the needs of the users while at the same time preserves their familiarity with the way traditional search engines operate. However, text-based image retrieval requires images to be annotated i.e. they are related to text information. Much effort has been invested on automatic image annotation methods [1], since the manual assignment of keywords (which is necessary for text-based image retrieval) is a time consuming and labour intensive procedure [2]. In automatic image annotation, a manually annotated set of data is used to train a system for the identification of joint or conditional probability of an annotation occurring together with a certain distribution of feature vectors corresponding to image content [3]. Different models and machine learning techniques were developed to learn the correlation between image features and textual words based on examples of annotated images. Learned models of this correlation are then applied to predict keywords for unseen images [4].  In the literature of automatic semantic image annotation, proposed approaches tend to classify images using only abstract terms or using holistic image features for both abstract terms and object classes. The extraction and selection of low-level features, either holistic or from particular image areas is of primary importance for automatic image annotation. This is true either for the content-based or for the text-based retrieval paradigm. In the former case the use of appropriate low-level features leads to accurate and effective object class models used in object detection while in the latter case, the better the low- level features are, the easier the learning of keyword models is. The intent of the image classification is to categorize the content of the input image to one of several keyword classes. A proper image annotation may contain more than one keyword that is relevant to the image content, so a reclassification process is required in this case, as well as whenever a new keyword class is added to the classification scheme. The creation of separate visual models for all keyword classes adds a significant value in automatic image annotation since several keywords can be assigned to the input image. As the number of keyword classes increases the number of keywords assigned to the images also increases too and there is no need for reclassification. However, the keyword modeling incurred various issues such as the large amount of manual effort required in developing the training data, the differences in interpretation of image contents, and the inconsistency of the keyword assignments among different annotators. This thesis focuses on image retrieval using keywords under the perspective of machine learning. It covers different aspects of the current research in this area including low-level feature extraction, creation of training sets and development of machine learning methodologies. It also proposes the idea of addressing automatic image annotation by creating visual models, one for each available keyword, and presents several examples of the proposed idea by comparing different features and machine learning algorithms in creating visual models for keywords referring to the athletics domain. The idea of automatic image annotation through independent keyword visual models is divided into two main parts: the training and automatic image annotation. In the first part, visual models for all available keywords are created, using the one-against-all training paradigm, while in the second part, annotations are produced for a given image based on the output of these models, once they are fed with a feature vector extracted from the input image. An accurate manually annotated dataset containing pairs of images and annotations is prerequisite for a successful automatic image annotation. Since the manual annotations are likely to contain human judgment errors and subjectivity in interpreting the image, the current thesis investigates the factors that influence the creation of manually annotated image datasets [5]. It also proposes the idea of modeling the knowledge of several people by creating visual models using such training data, aiming to significantly improve the ultimate efficiency of image retrieval systems [6]. Moreover, it proposes a new algorithm for the extraction of low level features. The Spatial Histogram of Keypoints (SHiK) [7], keeps the spatial information of localized keypoints, on an effort to overcome the limitations caused by the non-fixed and huge dimensionality of the SIFT feature vector when used in machine learning frameworks. SHiK partitions the image into a fixed number of ordered sub-regions based on the Hilbert space-Filling curve and counts the localized keypoints found inside each sub-region. The resulting spatial histogram is a compact and discriminative low-level feature vector that shows significantly improved performance on classification tasks. References [1] D. Zhang, M. M. Islam, G. Lu, “A review on automatic image annotation techniques”, Pattern Recognition, 45:346-362, 2012. [2] A. Hanbury, “A survey of methods for image annotation”, Journal of Visual Languages & Computing, 19(5):617-627, 2008. [3] K. Athanasakos, V. Stathopoulos, J. Jose, “A framework for evaluating automatic image annotation algorithms”, Lecture Notes in Computer Science, 5993:217-228, 2010. [4] R. Zhang, Z. Zhang, M. Li, H. J. Zhang, “A probabilistic semantic model for image annotation and multimodal image retrieval”, Multimedia Systems, pages 12(1):27-33, 2006. [5] Z. Theodosiou, N. Tsapatsoulis, “Semantic Gap Between People: An Experimental Investigation based on Image Annotation”, Proc. of the 7th International Workshop on Semantic Media Adaptation and Personalization, Luxembourg, 73-77, 2012. [6] Z. Theodosiou, N. Tsapatsoulis, “Modelling Crowdsourcing Originated Keywords within the Athletics Domain”, Artificial Intelligence Applications and Innovations, IFIP Advances in Information and Communication Technology, 381:404-413, 2012. [7] Z. Theodosiou, N. Tsapatsoulis, “Spatial Histogram of Keypoints (SHiK)”, Proc. of the IEEE International Conference on Image Processing, Melbourne 2924-2928, 2013.\n",
      "\n",
      "Cluster 1:\n",
      "- Clothing Genre Recognition System Using Image Processing Techniques- A Survey . Nowadays, Clothing business is one of the mostimportantcomponentsinthee-commerceindustry. So, there is plenty of online clothing sites are available where people can search and retrieve the most clothing items for their user query image. Clothing genre recognition is a very active topic in computer vision and multimedia research. In the textile industry, image processing techniques provide sensitive attention in the fieldoftheimage-basedclothingrecognitionsystem.The sequence of cloth images can be given as input to the recognition system. This clothing genre recognition system helps to detect the patterns and features of cloths which helps to classify them using effective feature extraction and classification algorithms. Feature extractiontechniquescanbeusedtoobtainfeaturesfrom thecloths.Classificationalgorithmsfromsoftcomputing help to automatically classify clothes genres depending on style elements and their salient visual features. Deep learning and Support Vector Machine (SVM) classifier achieved better performance in classifying both upper wear and lower wear genres. The main motivations of this paper focus on automatically classifying both upper wearandlowerweargenrefromafull-bodyinputimage. Evaluation metrics like precision, recall, F-score were used to measure the classification accuracy.This paper addresses on issues, challenges, applications, frameworks, tools, and techniques for recognition of clothing genres is carriedout.\n",
      "- A Melting Pot of Evolution and Learning We survey eight recent works by our group, involving the successful blending of evolutionary algorithms with machine learning and deep learning: 1. Binary and Multinomial Classification through Evolutionary Symbolic Regression, 2. Classy Ensemble: A Novel Ensemble Algorithm for Classification, 3. EC-KitY: Evolutionary Computation Tool Kit in Python, 4. Evolution of Activation Functions for Deep Learning-Based Image Classification, 5. Adaptive Combination of a Genetic Algorithm and Novelty Search for Deep Neuroevolution, 6. An Evolutionary, Gradient-Free, Query-Efficient, Black-Box Algorithm for Generating Adversarial Instances in Deep Networks, 7. Foiling Explanations in Deep Neural Networks, 8. Patch of Invisibility: Naturalistic Black-Box Adversarial Attacks on Object Detectors.\n",
      "- Multi-Label Active Learning Algorithms for Image Classification Image classification is a key task in image understanding, and multi-label image classification has become a popular topic in recent years. However, the success of multi-label image classification is closely related to the way of constructing a training set. As active learning aims to construct an effective training set through iteratively selecting the most informative examples to query labels from annotators, it was introduced into multi-label image classification. Accordingly, multi-label active learning is becoming an important research direction. In this work, we first review existing multi-label active learning algorithms for image classification. These algorithms can be categorized into two top groups from two aspects respectively: sampling and annotation. The most important component of multi-label active learning is to design an effective sampling strategy that actively selects the examples with the highest informativeness from an unlabeled data pool, according to various information measures. Thus, different informativeness measures are emphasized in this survey. Furthermore, this work also makes a deep investigation on existing challenging issues and future promises in multi-label active learning with a focus on four core aspects: example dimension, label dimension, annotation, and application extension.\n",
      "- Deep Active Learning in the Presence of Label Noise: A Survey Deep active learning has emerged as a powerful tool for training deep learning models within a predefined labeling budget. These models have achieved performances comparable to those trained in an offline setting. However, deep active learning faces substantial issues when dealing with classification datasets containing noisy labels. In this literature review, we discuss the current state of deep active learning in the presence of label noise, highlighting unique approaches, their strengths, and weaknesses. With the recent success of vision transformers in image classification tasks, we provide a brief overview and consider how the transformer layers and attention mechanisms can be used to enhance diversity, importance, and uncertainty-based selection in queries sent to an oracle for labeling. We further propose exploring contrastive learning methods to derive good image representations that can aid in selecting high-value samples for labeling in an active learning setting. We also highlight the need for creating unified benchmarks and standardized datasets for deep active learning in the presence of label noise for image classification to promote the reproducibility of research. The review concludes by suggesting avenues for future research in this area.\n",
      "- A survey on sentiment classification algorithms, challenges and applications Abstract Sentiment classification is the process of exploring sentiments, emotions, ideas and thoughts in the sentences which are expressed by the people. Sentiment classification allows us to judge the sentiments and feelings of the peoples by analyzing their reviews, social media comments etc. about all the aspects. Machine learning techniques and Lexicon based techniques are being mostly used in sentiment classification to predict sentiments from customers reviews and comments. Machine learning techniques includes several learning algorithms to judge the sentiments i.e Navie bayes, support vector machines etc whereas Lexicon Based techniques includes SentiWordnet, Wordnet etc. The main target of this survey is to give nearly full image of sentiment classification techniques. Survey paper provides the comprehensive overview of recent and past research on sentiment classification and provides excellent research queries and approaches for future aspects\n",
      "- Deep CNN and Deep GAN in Computational Visual Perception-Driven Image Analysis Computational visual perception, also known as computer vision, is a field of artificial intelligence that enables computers to process digital images and videos in a similar way as biological vision does. It involves methods to be developed to replicate the capabilities of biological vision. The computer vision’s goal is to surpass the capabilities of biological vision in extracting useful information from visual data. The massive data generated today is one of the driving factors for the tremendous growth of computer vision. This survey incorporates an overview of existing applications of deep learning in computational visual perception. The survey explores various deep learning techniques adapted to solve computer vision problems using deep convolutional neural networks and deep generative adversarial networks. The pitfalls of deep learning and their solutions are briefly discussed. The solutions discussed were dropout and augmentation. The results show that there is a significant improvement in the accuracy using dropout and data augmentation. Deep convolutional neural networks’ applications, namely, image classification, localization and detection, document analysis, and speech recognition, are discussed in detail. In-depth analysis of deep generative adversarial network applications, namely, image-to-image translation, image denoising, face aging, and facial attribute editing, is done. The deep generative adversarial network is unsupervised learning, but adding a certain number of labels in practical applications can improve its generating ability. However, it is challenging to acquire many data labels, but a small number of data labels can be acquired. Therefore, combining semisupervised learning and generative adversarial networks is one of the future directions. This article surveys the recent developments in this direction and provides a critical review of the related significant aspects, investigates the current opportunities and future challenges in all the emerging domains, and discusses the current opportunities in many emerging fields such as handwriting recognition, semantic mapping, webcam-based eye trackers, lumen center detection, query-by-string word, intermittently closed and open lakes and lagoons, and landslides.\n",
      "- Predicting Survival in Patients with Brain Tumors: Current State-of-the-Art of AI Methods Applied to MRI Given growing clinical needs, in recent years Artificial Intelligence (AI) techniques have increasingly been used to define the best approaches for survival assessment and prediction in patients with brain tumors. Advances in computational resources, and the collection of (mainly) public databases, have promoted this rapid development. This narrative review of the current state-of-the-art aimed to survey current applications of AI in predicting survival in patients with brain tumors, with a focus on Magnetic Resonance Imaging (MRI). An extensive search was performed on PubMed and Google Scholar using a Boolean research query based on MeSH terms and restricting the search to the period between 2012 and 2022. Fifty studies were selected, mainly based on Machine Learning (ML), Deep Learning (DL), radiomics-based methods, and methods that exploit traditional imaging techniques for survival assessment. In addition, we focused on two distinct tasks related to survival assessment: the first on the classification of subjects into survival classes (short and long-term or eventually short, mid and long-term) to stratify patients in distinct groups. The second focused on quantification, in days or months, of the individual survival interval. Our survey showed excellent state-of-the-art methods for the first, with accuracy up to ∼98%. The latter task appears to be the most challenging, but state-of-the-art techniques showed promising results, albeit with limitations, with C-Index up to ∼0.91. In conclusion, according to the specific task, the available computational methods perform differently, and the choice of the best one to use is non-univocal and dependent on many aspects. Unequivocally, the use of features derived from quantitative imaging has been shown to be advantageous for AI applications, including survival prediction. This evidence from the literature motivates further research in the field of AI-powered methods for survival prediction in patients with brain tumors, in particular, using the wealth of information provided by quantitative MRI techniques.\n",
      "- Active Learning to Assist Annotation of Aerial Images in Environmental Surveys Nowadays, remote sensing technologies greatly ease environmental assessment using aerial images. Such data are most often analyzed by a manual operator, leading to costly and non scalable solutions. In the fields of both machine learning and image processing, many algorithms have been developed to fasten and automate this complex task. Their main common assumption is the need to have prior ground truth available. However, for field experts or engineers, manually labeling the objects requires a time-consuming and tedious process. Restating the labeling issue as a binary classification one, we propose a method to assist the costly annotation task by introducing an active learning process, considering a query-by-group strategy. Assuming that a comprehensive context may be required to assist the annotator with the labeling task of a single instance, the labels of all the instances of an image are indeed queried. A score based on instances distribution is defined to rank the images for annotation and an appropriate retraining step is derived to simultaneously reduce the interaction cost and improve the classifier performances at each iteration. A numerical study on real images is conducted to assess the algorithm performances. It highlights promising results regarding the classification rate along with the chosen re-training strategy and the number of interactions with the user.\n",
      "- gamma-sky.net: Portal to the Gamma-Ray Sky this http URL is a novel interactive website designed for exploring the gamma-ray sky. The Map View portion of the site is powered by the Aladin Lite sky atlas, providing a scalable survey image tesselated onto a three-dimensional sphere. The map allows for interactive pan and zoom navigation as well as search queries by sky position or object name. The default image overlay shows the gamma-ray sky observed by the Fermi-LAT gamma-ray space telescope. Other survey images (e.g. Planck microwave images in low/high frequency bands, ROSAT X-ray image) are available for comparison with the gamma-ray data. Sources from major gamma-ray source catalogs of interest (Fermi-LAT 2FHL, 3FGL and a TeV source catalog) are overlaid over the sky map as markers. Clicking on a given source shows basic information in a popup, and detailed pages for every source are available via the Catalog View component of the website, including information such as source classification, spectrum and light-curve plots, and literature references. \n",
      "We intend for this http URL to be applicable for both professional astronomers as well as the general public. The website started in early June 2016 and is being developed as an open-source, open data project on GitHub (this https URL). We plan to extend it to display more gamma-ray and multi-wavelength data. Feedback and contributions are very welcome!\n",
      "\n",
      "Cluster 2:\n",
      "- Survey Towards Android Application for Plant Disease Detection using Deep Learning Approach India is a vast country where agriculture is primary occupation for people. Agriculture produce depends on crop and crop yield produced from farming. Crop yield majorly depends on growth and quality of plants. The plants need to be disease free to have good produce from farming. Taking same into consideration the system is designed to detect the plant disease in plants. The android application is created which is integrated with ai- chat and pesticide recommendation for farmer. The convolutional neural network is used for classification of plant disease. The application helps to recommend the pesticide to be used for specific disease detected. The farmers can use the application for any other query which is designed for chat. The application also includes the plant care calendar which helps farmer for regular plant care through which the growth and quality of plant can be maintained for high accuracy. Key Words: Android Application, Image Processing, Deep Learning ,Plant Disease, Plant care.\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# subproblems = json.loads(response.text)\n",
    "# year = date.today().year\n",
    "\n",
    "# results = search_semantic_scholar(subproblems[0][\"prompt\"], (year - 10, year))\n",
    "\n",
    "# max_results = 15  # Adjust this value based on expected results\n",
    "\n",
    "# for i, item in enumerate(results):\n",
    "#     if i >= len(results) or i >= max_results:\n",
    "#         break  # Exit when either all results are printed or the limit is reached\n",
    "#     print(item)\n",
    "\n",
    "#     time.sleep(1)\n",
    "\n",
    "\n",
    "import json\n",
    "import time\n",
    "from datetime import date\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming search_semantic_scholar is defined and retrieves papers based on a prompt\n",
    "# subproblems = json.loads(response.text)\n",
    "year = date.today().year\n",
    "\n",
    "subproblems = json.loads(response.text)  \n",
    "results = search_semantic_scholar(subproblems[0][\"prompt\"], (year - 10, year))\n",
    "\n",
    "max_results = 15  # Adjust this value based on expected results\n",
    "\n",
    "# Store the paper titles and abstracts\n",
    "papers = []\n",
    "\n",
    "for i, item in enumerate(results):\n",
    "    if i >= max_results or i >= len(results):\n",
    "        break  # Exit when the limit is reached\n",
    "    title = item.title if hasattr(item, 'title') else \"No Title\"\n",
    "    abstract = item.abstract if hasattr(item, 'abstract') else \"No Abstract\"\n",
    "    papers.append(f\"{title} {abstract}\")  # Combine title and abstract for better context\n",
    "\n",
    "    print(title)  # Optionally print the title\n",
    "    time.sleep(1)\n",
    "\n",
    "# Clustering the papers based on techniques extracted from titles and abstracts\n",
    "def extract_techniques(documents):\n",
    "    # Placeholder for a more sophisticated technique extraction\n",
    "    techniques = set()\n",
    "    for doc in documents:\n",
    "        words = doc.split()\n",
    "        for word in words:\n",
    "            if word.lower() in ['technique', 'method', 'approach', 'algorithm', 'framework']:\n",
    "                techniques.add(word)\n",
    "    return list(techniques)\n",
    "\n",
    "# Vectorize the papers\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(papers)\n",
    "\n",
    "# Perform K-Means clustering\n",
    "num_clusters = 3  # You can adjust this based on your dataset\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Create a DataFrame to hold papers and their cluster labels\n",
    "df = pd.DataFrame({'Paper': papers, 'Cluster': kmeans.labels_})\n",
    "\n",
    "# Output the clustered papers\n",
    "print(df)\n",
    "\n",
    "# Optionally, you can print papers under each cluster\n",
    "for cluster in range(num_clusters):\n",
    "    print(f\"\\nCluster {cluster}:\")\n",
    "    cluster_papers = df[df['Cluster'] == cluster]['Paper'].tolist()\n",
    "    for paper in cluster_papers:\n",
    "        print(f\"- {paper}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0:\n",
      "Words that are matched the most: image, retrieval, cbir, content, based\n",
      "\n",
      "Cluster 1:\n",
      "Words that are matched the most: learning, deep, active, label, classification\n",
      "\n",
      "Cluster 2:\n",
      "Words that are matched the most: plant, disease, application, android, care\n"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "# # Load a pre-trained NER model\n",
    "# nlp = spacy.load(\"en_core_sci_sm\")\n",
    "\n",
    "# papers = [result[\"title\"] + \" \" + result[\"abstract\"] + \" \" + result[\"tldr\"] for result in results]\n",
    "\n",
    "# techniques = []\n",
    "# for paper in papers:\n",
    "#     doc = nlp(paper)\n",
    "#     techniques_in_paper = [ent.text for ent in doc.ents if ent.label_ == \"TECHNIQUE\"]\n",
    "#     techniques.append(\" \".join(techniques_in_paper))\n",
    "# Get the cluster centroids\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Get the feature names (techniques/words)\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "# For each cluster, print the top terms (techniques) contributing to that cluster\n",
    "num_top_techniques = 5  # Number of top techniques to display for each cluster\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(f\"\\nCluster {i}:\")\n",
    "    \n",
    "    # Sort the terms based on their importance in the cluster centroid\n",
    "    sorted_tech_indices = centroids[i].argsort()[::-1][:num_top_techniques]\n",
    "    \n",
    "    # Print the top terms (techniques) for this cluster\n",
    "    top_techniques = [terms[idx] for idx in sorted_tech_indices]\n",
    "    print(\"Words that are matched the most:\", \", \".join(top_techniques))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
