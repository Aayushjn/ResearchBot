{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def search_semantic_scholar(query, year_start, year_end):\n",
    "#     url = \"https://api.semanticscholar.org/v1/paper/search\"\n",
    "#     params = {\n",
    "#         \"query\": query,\n",
    "#         \"yearStart\": year_start,\n",
    "#         \"yearEnd\": year_end,\n",
    "#         \"openAccessPdf\": True,  # Optional: Filter for open access papers\n",
    "#         \"fieldsOfStudy\": [\"Computer Science\"],  # Adjust based on your field\n",
    "#         \"fields\": [\"paperId\", \"title\", \"abstract\", \"tldr\"],  # Fields to retrieve\n",
    "#         \"limit\": 15  # Adjust the maximum number of results\n",
    "#     }\n",
    "#     response = requests.get(url, params=params)\n",
    "#     if response.status_code == 200:\n",
    "#         return response.json()\n",
    "#     else:\n",
    "#         print(\"Error:\", response.status_code)\n",
    "#         return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from sklearn.cluster 1  import KMeans\n",
    "\n",
    "# # ... (existing code for searching papers)\n",
    "\n",
    "# # Feature extraction with TF-IDF\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# features = vectorizer.fit_transform([result[\"title\"] + \" \" + result[\"abstract\"] + \" \" + result[\"tldr\"] for result in results])\n",
    "\n",
    "# # Similarity matrix\n",
    "# similarity_matrix = cosine_similarity(features)\n",
    "\n",
    "# # Clustering with KMeans (adjust number of clusters as needed)\n",
    "# kmeans = KMeans(n_clusters=3)\n",
    "# kmeans.fit(features)\n",
    "\n",
    "# # Assign each paper to a cluster\n",
    "# clusters = kmeans.labels_\n",
    "\n",
    "# # Print paper information with assigned cluster\n",
    "# for i, result in enumerate(results):\n",
    "#   print(f\"Paper: {result['title']}, Cluster: {clusters[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from pprint import pprint\n",
    "from typing_extensions import TypedDict\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from semanticscholar import SemanticScholar\n",
    "import google.generativeai as genai\n",
    "from google.oauth2 import service_account\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path=\"API_key.env\")\n",
    "service_account_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "credentials = service_account.Credentials.from_service_account_file(service_account_key)\n",
    "genai.configure(credentials=credentials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "wait_fixed.__init__() got an unexpected keyword argument 'seconds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m sch \u001b[38;5;241m=\u001b[39m SemanticScholar(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtenacity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m retry, stop_after_attempt, wait_fixed\n\u001b[0;32m      5\u001b[0m \u001b[38;5;129m@retry\u001b[39m(\n\u001b[0;32m      6\u001b[0m     stop\u001b[38;5;241m=\u001b[39mstop_after_attempt(\u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m----> 7\u001b[0m     wait\u001b[38;5;241m=\u001b[39m\u001b[43mwait_fixed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m# Correct usage of wait_fixed\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     retry_if\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m e: \u001b[38;5;28misinstance\u001b[39m(e, \u001b[38;5;167;01mConnectionRefusedError\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch_semantic_scholar\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m, year: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Searches Semantic Scholar for papers with retry logic.\"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sch\u001b[38;5;241m.\u001b[39msearch_paper(\n\u001b[0;32m     13\u001b[0m         query\u001b[38;5;241m=\u001b[39mquery,\n\u001b[0;32m     14\u001b[0m         year\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m,\n\u001b[0;32m     19\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: wait_fixed.__init__() got an unexpected keyword argument 'seconds'"
     ]
    }
   ],
   "source": [
    "sch = SemanticScholar(timeout=15)\n",
    "\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "\n",
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_fixed(seconds=10),  # Correct usage of wait_fixed\n",
    "    retry_if=lambda e: isinstance(e, ConnectionRefusedError)\n",
    ")\n",
    "def search_semantic_scholar(query: str, year: tuple[int, int]) -> list:\n",
    "    \"\"\"Searches Semantic Scholar for papers with retry logic.\"\"\"\n",
    "    return sch.search_paper(\n",
    "        query=query,\n",
    "        year=f\"{year[0]}-{year[1]}\",\n",
    "        open_access_pdf=True,\n",
    "        fields_of_study=[\"Computer Science\"],\n",
    "        fields=[\"paperId\", \"title\", \"abstract\", \"tldr\", \"openAccessPdf\"],\n",
    "        limit=15,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_techniques(paper):\n",
    "    \"\"\"Extracts techniques from a paper's title, abstract, and TLDR.\"\"\"\n",
    "    text = paper[\"title\"] + \" \" + paper[\"abstract\"] + \" \" + paper[\"tldr\"]\n",
    "    # Use NLP techniques (e.g., named entity recognition, keyword extraction) to extract techniques\n",
    "    techniques = extract_keywords(text)  # Replace with your keyword extraction function\n",
    "    return techniques\n",
    "\n",
    "def cluster_papers(papers):\n",
    "    \"\"\"Clusters papers based on extracted techniques.\"\"\"\n",
    "    texts = [extract_techniques(paper) for paper in papers]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "    # Use KMeans clustering with 4 clusters (highly relevant, moderately relevant, slightly relevant, irrelevant)\n",
    "    kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "    kmeans.fit(tfidf_matrix)\n",
    "    cluster_labels = kmeans.labels_\n",
    "\n",
    "    # Organize papers into clusters based on labels\n",
    "    categories = {i: [] for i in range(4)}\n",
    "    for paper, label in zip(papers, cluster_labels):\n",
    "        categories[label].append(paper)\n",
    "\n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "You are a Computer Science PhD student. Your goal is to write review/survey papers in specific areas of Computer Science. You should be able to:\n",
    "- Identify research problems and break them down into sub problems\n",
    "- Conduct thorough literature review on your topic, summarize key findings and identify gaps in existing methodologies\n",
    "- Formulate clear and testable hypotheses to address your research questions\n",
    "- Develop experimental methodologies to test your hypotheses, considering factors such as data collection, analysis, and evaluation\n",
    "- Collect, clean and analyze relevant data using appropriate tools and techniques\n",
    "- Draw meaningful conclusions from your research findings and discuss their implications\n",
    "- Prepare high-quality research papers that effectively communicate your findings\n",
    "\"\"\"\n",
    "model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", system_instruction=system_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subproblem(TypedDict):\n",
    "    prompt: str\n",
    "    requires_internet: bool\n",
    "    requires_previous_output: bool\n",
    "\n",
    "\n",
    "topic = \"Augmented reality\"\n",
    "prompt = f\"\"\"\n",
    "You are researching the below topic and need to write a survey paper on the same. Your current goal is only to research on the topic \n",
    "and not write anything currently.\n",
    "\n",
    "Topic: {topic}\n",
    "\n",
    "Instructions:\n",
    "- Identify the key areas of focus within this topic and outline the subproblems that need to be addressed\n",
    "- For each subproblem, create a concise prompt that states the task to be performed\n",
    "- Indicate whether internet access is necessary to complete the subproblem. Assume that around 15-20 relevant research papers will be provided to you.\n",
    "- Determine if the output of the previous subproblem is relevant to the subsequent subproblem\n",
    "- Your first subproblem should always be a query string that can be used to find relevant research papers from the Semantic Scholar database\n",
    "\n",
    "Do not generate the same instructions as your output. Ensure that you provide relevant subproblems that can be addressed by you.\n",
    "Ensure that your output is in the correct format since it will be parsed automatically.\n",
    "\"\"\"\n",
    "\n",
    "gen_config = {\n",
    "    \"response_mime_type\": \"application/json\",\n",
    "    \"response_schema\": list[Subproblem]\n",
    "}\n",
    "\n",
    "response = model.generate_content(prompt, generation_config=gen_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subproblems = json.loads(response.text)\n",
    "year = date.today().year\n",
    "\n",
    "results = search_semantic_scholar(subproblems[0][\"prompt\"], (year - 10, year))\n",
    "\n",
    "max_results = 15  # Adjust this value based on expected results\n",
    "\n",
    "\n",
    "# Categorize papers\n",
    "categories = cluster_papers(results)\n",
    "\n",
    "# Print results\n",
    "for category, papers in categories.items():\n",
    "    print(f\"Category: {category}\")\n",
    "    for paper in papers:\n",
    "        print(f\"- {paper['title']}\")\n",
    "\n",
    "        \n",
    "# for i, item in enumerate(results):\n",
    "#     if i >= len(results) or i >= max_results:\n",
    "#         break  # Exit when either all results are printed or the limit is reached\n",
    "#     print(item)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
